{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X__s-8C1q1na"
      },
      "outputs": [],
      "source": [
        "# MACHINE LEARNING MODELS WITH SKLEARN PIPELINES\n",
        "# Rossmann Sales Forecasting - Task 2: Machine Learning Pipeline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import logging\n",
        "import pickle\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression,Ridge, Lasso\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
        "import sys\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from scipy import stats\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "# Clear any existing handlers\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "# Configure logging with force=True to override existing config\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    force=True  # This is key for Colab\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_53y62001gOJ",
        "outputId": "a9611428-74d7-4930-fcb6-470b8171e781"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting mlflow-skinny==3.1.1 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.1.1-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading databricks_sdk-0.57.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.115.12)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.11.7)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (4.14.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.34.3)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.46.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (2025.6.15)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n",
            "Downloading mlflow-3.1.1-py3-none-any.whl (24.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.1.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.57.0-py3-none-any.whl (733 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gunicorn, graphql-core, opentelemetry-api, graphql-relay, docker, alembic, opentelemetry-semantic-conventions, graphene, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
            "Successfully installed alembic-1.16.2 databricks-sdk-0.57.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-3.1.1 mlflow-skinny-3.1.1 opentelemetry-api-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ExaQErdvtlRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc01baa-9993-4662-a618-baa381b6f2f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7zIMlFjmvKon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b9ba31-c781-471a-e314-99e86f2a7887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-25 11:29:43,328 - INFO - Train data loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Use the correct path to your file in Google Drive\n",
        "file_path = '/content/drive/MyDrive/Colab_Notebooks/processed_data_set/train_cleaned.csv'\n",
        "# Load the CSV into a variable\n",
        "train_df = pd.read_csv(file_path)\n",
        "logging.info(\"Train data loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FT0LbgfsYkZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df2cd8d8-c698-4157-9adf-3c4bd145b678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-25 11:29:44,638 - INFO - Loaded training data with shape: (1017155, 27)\n"
          ]
        }
      ],
      "source": [
        "logging.info(f\"Loaded training data with shape: {train_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_6ss-VvzY4VP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "751e3dbc-2cb0-47c7-cbe5-92045364878b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-25 11:29:45,897 - INFO - Starting preprocessing pipeline\n"
          ]
        }
      ],
      "source": [
        "# 2.1 PREPROCESSING\n",
        "logging.info(\"Starting preprocessing pipeline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pEeLXHq2ZRrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "731a349e-96fb-4614-a879-74bf46d6ca57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-25 11:29:46,878 - INFO - Creating new features...\n"
          ]
        }
      ],
      "source": [
        "# Feature Engineering\n",
        "logging.info(\"Creating new features...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JiZ9_C2mZVLp"
      },
      "outputs": [],
      "source": [
        "# Convert Date column to datetime format first\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "\n",
        "# Date features\n",
        "train_df['Year'] = train_df['Date'].dt.year\n",
        "train_df['Month'] = train_df['Date'].dt.month\n",
        "train_df['Day'] = train_df['Date'].dt.day\n",
        "train_df['DayOfWeek'] = train_df['Date'].dt.dayofweek\n",
        "train_df['WeekOfYear'] = train_df['Date'].dt.isocalendar().week\n",
        "train_df['Quarter'] = train_df['Date'].dt.quarter\n",
        "train_df['IsWeekend'] = train_df['DayOfWeek'].isin([5, 6]).astype(int)\n",
        "\n",
        "# Holiday features\n",
        "train_df['IsStateHoliday'] = (train_df['StateHoliday'] != '0').astype(int)\n",
        "train_df['IsSchoolHoliday'] = train_df['SchoolHoliday'].astype(int)\n",
        "\n",
        "# Create a reference date for holiday calculations\n",
        "reference_date = train_df['Date'].min()\n",
        "\n",
        "# Days to/from holidays calculation\n",
        "state_holidays = train_df[train_df['IsStateHoliday'] == 1]['Date'].unique()\n",
        "train_df['DaysToHoliday'] = np.nan\n",
        "train_df['DaysAfterHoliday'] = np.nan\n",
        "\n",
        "for idx, row in train_df.iterrows():\n",
        "    current_date = row['Date']\n",
        "\n",
        "    # Find nearest future holiday\n",
        "    future_holidays = state_holidays[state_holidays > current_date]\n",
        "    if len(future_holidays) > 0:\n",
        "        days_to = (future_holidays.min() - current_date).days\n",
        "        train_df.loc[idx, 'DaysToHoliday'] = min(days_to, 30)  # Cap at 30 days\n",
        "    else:\n",
        "        train_df.loc[idx, 'DaysToHoliday'] = 30\n",
        "\n",
        "    # Find nearest past holiday\n",
        "    past_holidays = state_holidays[state_holidays < current_date]\n",
        "    if len(past_holidays) > 0:\n",
        "        days_after = (current_date - past_holidays.max()).days\n",
        "        train_df.loc[idx, 'DaysAfterHoliday'] = min(days_after, 30)  # Cap at 30 days\n",
        "    else:\n",
        "        train_df.loc[idx, 'DaysAfterHoliday'] = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "D5G0JdrsZcDK"
      },
      "outputs": [],
      "source": [
        "# Month position features\n",
        "train_df['IsBeginningOfMonth'] = (train_df['Day'] <= 10).astype(int)\n",
        "train_df['IsMidMonth'] = ((train_df['Day'] > 10) & (train_df['Day'] <= 20)).astype(int)\n",
        "train_df['IsEndOfMonth'] = (train_df['Day'] > 20).astype(int)\n",
        "\n",
        "# Competition features\n",
        "train_df['CompetitionAge'] = train_df['Year'] - train_df['CompetitionOpenSinceYear']\n",
        "train_df['CompetitionAge'] = train_df['CompetitionAge'].clip(lower=0)\n",
        "train_df['HasCompetition'] = (train_df['CompetitionDistance'].notna()).astype(int)\n",
        "\n",
        "# Promo features\n",
        "train_df['PromoAge'] = np.where(\n",
        "    train_df['Promo2'] == 1,\n",
        "    (train_df['Year'] - train_df['Promo2SinceYear']) * 52 +\n",
        "    (train_df['WeekOfYear'] - train_df['Promo2SinceWeek']),\n",
        "    0\n",
        ")\n",
        "train_df['PromoAge'] = train_df['PromoAge'].clip(lower=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJGfZrBEbMDs",
        "outputId": "c5d5c28d-6ca4-4cae-dae3-ec54aa60f9c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-25 11:54:45,105 - INFO - Feature engineering completed\n"
          ]
        }
      ],
      "source": [
        "# Seasonal features\n",
        "train_df['IsSummer'] = train_df['Month'].isin([6, 7, 8]).astype(int)\n",
        "train_df['IsWinter'] = train_df['Month'].isin([12, 1, 2]).astype(int)\n",
        "train_df['IsSpring'] = train_df['Month'].isin([3, 4, 5]).astype(int)\n",
        "train_df['IsAutumn'] = train_df['Month'].isin([9, 10, 11]).astype(int)\n",
        "\n",
        "# Sales per customer (for stores that are open)\n",
        "train_df['SalesPerCustomer'] = np.where(\n",
        "    train_df['Customers'] > 0,\n",
        "    train_df['Sales'] / train_df['Customers'],\n",
        "    0\n",
        ")\n",
        "\n",
        "logging.info(\"Feature engineering completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8KLl94ebXhG",
        "outputId": "a2380c51-aad0-4c85-f312-e4d0b12c6420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-25 11:54:46,329 - INFO - Encoding categorical variables\n"
          ]
        }
      ],
      "source": [
        "# Encode categorical variables\n",
        "logging.info(\"Encoding categorical variables\")\n",
        "\n",
        "# Label encoding for categorical variables\n",
        "le_store_type = LabelEncoder()\n",
        "le_assortment = LabelEncoder()\n",
        "le_state_holiday = LabelEncoder()\n",
        "le_promo_interval = LabelEncoder()\n",
        "\n",
        "train_df['StoreType_encoded'] = le_store_type.fit_transform(train_df['StoreType'])\n",
        "train_df['Assortment_encoded'] = le_assortment.fit_transform(train_df['Assortment'])\n",
        "train_df['StateHoliday_encoded'] = le_state_holiday.fit_transform(train_df['StateHoliday'].astype(str))\n",
        "train_df['PromoInterval_encoded'] = le_promo_interval.fit_transform(train_df['PromoInterval'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vlTw62qCbbX3"
      },
      "outputs": [],
      "source": [
        "# Handle missing values\n",
        "train_df['CompetitionDistance'].fillna(train_df['CompetitionDistance'].median(), inplace=True)\n",
        "train_df['CompetitionOpenSinceMonth'].fillna(0, inplace=True)\n",
        "train_df['CompetitionOpenSinceYear'].fillna(1900, inplace=True)\n",
        "train_df['Promo2SinceWeek'].fillna(0, inplace=True)\n",
        "train_df['Promo2SinceYear'].fillna(1900, inplace=True)\n",
        "train_df['DaysToHoliday'].fillna(30, inplace=True)\n",
        "train_df['DaysAfterHoliday'].fillna(30, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4qgbhUABbgGb"
      },
      "outputs": [],
      "source": [
        "# Select features for modeling\n",
        "feature_columns = [\n",
        "    'Store', 'DayOfWeek', 'Open', 'Promo', 'SchoolHoliday', 'StoreType_encoded',\n",
        "    'Assortment_encoded', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
        "    'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear',\n",
        "    'PromoInterval_encoded', 'Year', 'Month', 'Day', 'WeekOfYear', 'Quarter',\n",
        "    'IsWeekend', 'IsStateHoliday', 'IsSchoolHoliday', 'DaysToHoliday',\n",
        "    'DaysAfterHoliday', 'IsBeginningOfMonth', 'IsMidMonth', 'IsEndOfMonth',\n",
        "    'CompetitionAge', 'HasCompetition', 'PromoAge', 'IsSummer', 'IsWinter',\n",
        "    'IsSpring', 'IsAutumn'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5QX1t32bjRg",
        "outputId": "f9e07678-0959-4d55-ce06-7544dec54221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-25 11:54:51,624 - INFO - Training on 844338 open store records\n",
            "2025-06-25 11:54:52,020 - INFO - Train set size: (675470, 34), Test set size: (168868, 34)\n"
          ]
        }
      ],
      "source": [
        "# Filter out rows where store is closed (Sales = 0, Open = 0)\n",
        "train_df_filtered = train_df[train_df['Open'] == 1].copy()\n",
        "logging.info(f\"Training on {len(train_df_filtered)} open store records\")\n",
        "\n",
        "X = train_df_filtered[feature_columns]\n",
        "y = train_df_filtered['Sales']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "logging.info(f\"Train set size: {X_train.shape}, Test set size: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIPVUUi8bojc",
        "outputId": "f4e54427-bc66-40f6-86e4-19cc9b5db2aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-25 11:54:54,200 - INFO - Building ML models with sklearn pipelines\n"
          ]
        }
      ],
      "source": [
        "# 2.2 BUILDING MODELS WITH SKLEARN PIPELINES\n",
        "logging.info(\"Building ML models with sklearn pipelines\")\n",
        "\n",
        "# Define preprocessing pipeline\n",
        "preprocessing_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Define models to try\n",
        "models = {\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "    'Linear Regression': LinearRegression()\n",
        "}\n",
        "\n",
        "# Create pipelines for each model\n",
        "pipelines = {}\n",
        "for name, model in models.items():\n",
        "    pipelines[name] = Pipeline([\n",
        "        ('preprocessing', preprocessing_pipeline),\n",
        "        ('model', model)\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKThr6qzbsjG",
        "outputId": "7d2ec191-0970-4016-95d6-165b874430fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-25 11:54:56,136 - INFO - Evaluating models with multiple metrics\n"
          ]
        }
      ],
      "source": [
        "# 2.3 LOSS FUNCTION CHOICE\n",
        "logging.info(\"Evaluating models with multiple metrics\")\n",
        "\n",
        "# Custom loss functions\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calculate MAPE\"\"\"\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def root_mean_squared_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calculate RMSPE - commonly used for sales forecasting\"\"\"\n",
        "    return np.sqrt(np.mean(((y_true - y_pred) / y_true) ** 2)) * 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wyd5mcrbvyD",
        "outputId": "fc0c8fb9-ca1b-4134-c689-2e0305949b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-25 11:54:57,435 - INFO - Training and evaluating models...\n",
            "2025-06-25 11:54:57,437 - INFO - Training Random Forest...\n"
          ]
        }
      ],
      "source": [
        "# Evaluate models\n",
        "model_results = {}\n",
        "logging.info(\"Training and evaluating models...\")\n",
        "\n",
        "for name, pipeline in pipelines.items():\n",
        "    logging.info(f\"Training {name}...\")\n",
        "\n",
        "    # Fit the pipeline\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_train = pipeline.predict(X_train)\n",
        "    y_pred_test = pipeline.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    results = {\n",
        "        'train_mse': mean_squared_error(y_train, y_pred_train),\n",
        "        'test_mse': mean_squared_error(y_test, y_pred_test),\n",
        "        'train_mae': mean_absolute_error(y_train, y_pred_train),\n",
        "        'test_mae': mean_absolute_error(y_test, y_pred_test),\n",
        "        'train_r2': r2_score(y_train, y_pred_train),\n",
        "        'test_r2': r2_score(y_test, y_pred_test),\n",
        "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
        "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
        "        'train_mape': mean_absolute_percentage_error(y_train, y_pred_train),\n",
        "        'test_mape': mean_absolute_percentage_error(y_test, y_pred_test),\n",
        "        'train_rmspe': root_mean_squared_percentage_error(y_train, y_pred_train),\n",
        "        'test_rmspe': root_mean_squared_percentage_error(y_test, y_pred_test)\n",
        "    }\n",
        "\n",
        "    model_results[name] = results\n",
        "\n",
        "    logging.info(f\"{name} - Test RMSE: {results['test_rmse']:.2f}, Test RMSPE: {results['test_rmspe']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Rm-f3bmb0k4"
      },
      "outputs": [],
      "source": [
        "# Display results\n",
        "results_df = pd.DataFrame(model_results).T\n",
        "print(\"\\n=== MODEL COMPARISON ===\")\n",
        "print(results_df[['test_rmse', 'test_rmspe', 'test_r2', 'test_mape']].round(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBLYWpuWb4Gs"
      },
      "outputs": [],
      "source": [
        "# Select best model based on RMSPE (Root Mean Squared Percentage Error)\n",
        "best_model_name = results_df['test_rmspe'].idxmin()\n",
        "best_pipeline = pipelines[best_model_name]\n",
        "logging.info(f\"Best model: {best_model_name} with RMSPE: {results_df.loc[best_model_name, 'test_rmspe']:.3f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def progressive_tuning(pipeline, X_train, y_train, best_model_name):\n",
        "    \"\"\"Tune hyperparameters progressively to save time with detailed logging\"\"\"\n",
        "\n",
        "    logging.info(f\"Starting progressive hyperparameter tuning for {best_model_name}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    if best_model_name == 'Random Forest':\n",
        "        logging.info(\"=== RANDOM FOREST PROGRESSIVE TUNING ===\")\n",
        "\n",
        "        #  Tune n_estimators first\n",
        "        logging.info(\" Tuning n_estimators...\")\n",
        "        param_grid_1 = {'model__n_estimators': [50, 100, 200]}\n",
        "        logging.info(f\"Testing parameters: {param_grid_1}\")\n",
        "\n",
        "        stage1_start = time.time()\n",
        "        grid_1 = GridSearchCV(\n",
        "            pipeline,\n",
        "            param_grid_1,\n",
        "            cv=3,\n",
        "            scoring='neg_mean_squared_error',\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "        grid_1.fit(X_train, y_train)\n",
        "        stage1_time = time.time() - stage1_start\n",
        "\n",
        "        best_n_est = grid_1.best_params_['model__n_estimators']\n",
        "        best_score_1 = grid_1.best_score_\n",
        "\n",
        "        logging.info(f\" completed  {stage1_time:.2f} seconds\")\n",
        "        logging.info(f\"Best n_estimators: {best_n_est}\")\n",
        "        logging.info(f\"Best CV score: {best_score_1:.4f}\")\n",
        "        logging.info(f\"All  scores: {dict(zip([str(p) for p in param_grid_1['model__n_estimators']], grid_1.cv_results_['mean_test_score']))}\")\n",
        "\n",
        "        #  Fix best n_estimators, tune other params\n",
        "        logging.info(\"\\n Tuning max_depth and min_samples_split...\")\n",
        "        param_grid_2 = {\n",
        "            'model__n_estimators': [best_n_est],\n",
        "            'model__max_depth': [10, 20, None],\n",
        "            'model__min_samples_split': [2, 5]\n",
        "        }\n",
        "        logging.info(f\"Testing parameters: {param_grid_2}\")\n",
        "\n",
        "        stage2_start = time.time()\n",
        "        grid_2 = GridSearchCV(\n",
        "            pipeline,\n",
        "            param_grid_2,\n",
        "            cv=3,\n",
        "            scoring='neg_mean_squared_error',\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "        grid_2.fit(X_train, y_train)\n",
        "        stage2_time = time.time() - stage2_start\n",
        "\n",
        "        best_score_2 = grid_2.best_score_\n",
        "\n",
        "        logging.info(f\" completed  {stage2_time:.2f} seconds\")\n",
        "        logging.info(f\"Final best parameters: {grid_2.best_params_}\")\n",
        "        logging.info(f\"Final best CV score: {best_score_2:.4f}\")\n",
        "        logging.info(f\"Score improvement : {best_score_2 - best_score_1:.4f}\")\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        logging.info(f\"Random Forest progressive tuning completed in {total_time:.2f} seconds\")\n",
        "\n",
        "        return grid_2.best_estimator_\n",
        "\n",
        "    elif best_model_name == 'Gradient Boosting':\n",
        "        logging.info(\"=== GRADIENT BOOSTING PROGRESSIVE TUNING ===\")\n",
        "\n",
        "        # Tune learning_rate and n_estimators\n",
        "        logging.info(\" Tuning learning_rate and n_estimators...\")\n",
        "        param_grid_1 = {\n",
        "            'model__learning_rate': [0.05, 0.1, 0.2],\n",
        "            'model__n_estimators': [100, 200]\n",
        "        }\n",
        "        logging.info(f\"Testing parameters: {param_grid_1}\")\n",
        "\n",
        "        stage1_start = time.time()\n",
        "        grid_1 = GridSearchCV(\n",
        "            pipeline,\n",
        "            param_grid_1,\n",
        "            cv=3,\n",
        "            scoring='neg_mean_squared_error',\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "        grid_1.fit(X_train, y_train)\n",
        "        stage1_time = time.time() - stage1_start\n",
        "\n",
        "        best_lr = grid_1.best_params_['model__learning_rate']\n",
        "        best_n_est = grid_1.best_params_['model__n_estimators']\n",
        "        best_score_1 = grid_1.best_score_\n",
        "\n",
        "        logging.info(f\"Stage 1 completed in {stage1_time:.2f} seconds\")\n",
        "        logging.info(f\"Best learning_rate: {best_lr}\")\n",
        "        logging.info(f\"Best n_estimators: {best_n_est}\")\n",
        "        logging.info(f\"Best CV score: {best_score_1:.4f}\")\n",
        "\n",
        "        # Log all combinations tested\n",
        "        results_1 = []\n",
        "        for i, (lr, n_est) in enumerate([(lr, n_est) for lr in param_grid_1['model__learning_rate']\n",
        "                                        for n_est in param_grid_1['model__n_estimators']]):\n",
        "            score = grid_1.cv_results_['mean_test_score'][i]\n",
        "            results_1.append(f\"lr={lr}, n_est={n_est}: {score:.4f}\")\n",
        "        logging.info(f\"All Stage 1 results: {results_1}\")\n",
        "\n",
        "        #  Fix best lr and n_est, tune depth\n",
        "        logging.info(\"\\n Tuning max_depth...\")\n",
        "        param_grid_2 = {\n",
        "            'model__learning_rate': [best_lr],\n",
        "            'model__n_estimators': [best_n_est],\n",
        "            'model__max_depth': [3, 6, 10]\n",
        "        }\n",
        "        logging.info(f\"Testing parameters: {param_grid_2}\")\n",
        "\n",
        "        stage2_start = time.time()\n",
        "        grid_2 = GridSearchCV(\n",
        "            pipeline,\n",
        "            param_grid_2,\n",
        "            cv=3,\n",
        "            scoring='neg_mean_squared_error',\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "        grid_2.fit(X_train, y_train)\n",
        "        stage2_time = time.time() - stage2_start\n",
        "\n",
        "        best_score_2 = grid_2.best_score_\n",
        "\n",
        "        logging.info(f\"Stage 2 completed in {stage2_time:.2f} seconds\")\n",
        "        logging.info(f\"Final best parameters: {grid_2.best_params_}\")\n",
        "        logging.info(f\"Final best CV score: {best_score_2:.4f}\")\n",
        "        logging.info(f\"Score improvement from Stage 1: {best_score_2 - best_score_1:.4f}\")\n",
        "\n",
        "        # Log all depth results\n",
        "        depth_results = dict(zip([3, 6, 10], grid_2.cv_results_['mean_test_score']))\n",
        "        logging.info(f\"All depth results: {depth_results}\")\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        logging.info(f\"Gradient Boosting progressive tuning completed in {total_time:.2f} seconds\")\n",
        "\n",
        "        return grid_2.best_estimator_\n",
        "\n",
        "    else:\n",
        "        logging.warning(f\"Progressive tuning not implemented for {best_model_name}\")\n",
        "        logging.info(\"Returning original pipeline without tuning\")\n",
        "        return pipeline"
      ],
      "metadata": {
        "id": "B7K8FYMzlvvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDEivjw2b-fY"
      },
      "outputs": [],
      "source": [
        "# Final model evaluation\n",
        "y_pred_final = best_pipeline.predict(X_test)\n",
        "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
        "final_rmspe = root_mean_squared_percentage_error(y_test, y_pred_final)\n",
        "final_r2 = r2_score(y_test, y_pred_final)\n",
        "\n",
        "logging.info(f\"Final model performance - RMSE: {final_rmse:.2f}, RMSPE: {final_rmspe:.3f}%, R²: {final_r2:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-6OBwKbcApx"
      },
      "outputs": [],
      "source": [
        "# 2.4 POST PREDICTION ANALYSIS\n",
        "logging.info(\"Performing post-prediction analysis...\")\n",
        "\n",
        "# Feature importance (for tree-based models)\n",
        "if hasattr(best_pipeline.named_steps['model'], 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_columns,\n",
        "        'importance': best_pipeline.named_steps['model'].feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.barh(feature_importance.head(15)['feature'], feature_importance.head(15)['importance'])\n",
        "    plt.title('Top 15 Feature Importances')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n=== TOP 10 FEATURE IMPORTANCES ===\")\n",
        "    print(feature_importance.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unD6vPa-cDLn"
      },
      "outputs": [],
      "source": [
        "# Residual analysis\n",
        "residuals = y_test - y_pred_final\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.scatter(y_pred_final, residuals, alpha=0.5)\n",
        "plt.xlabel('Predicted Sales')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Predicted')\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.hist(residuals, bins=50, edgecolor='black')\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Residuals Distribution')\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "plt.title('Q-Q Plot')\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.scatter(y_test, y_pred_final, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel('Actual Sales')\n",
        "plt.ylabel('Predicted Sales')\n",
        "plt.title('Actual vs Predicted')\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.scatter(range(len(residuals)), residuals, alpha=0.5)\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Index')\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "\n",
        "plt.subplot(2, 3, 6)\n",
        "# Error by day of week\n",
        "error_by_dow = pd.DataFrame({\n",
        "    'DayOfWeek': X_test['DayOfWeek'],\n",
        "    'Error': np.abs(residuals)\n",
        "}).groupby('DayOfWeek')['Error'].mean()\n",
        "plt.bar(range(7), error_by_dow.values)\n",
        "plt.xlabel('Day of Week (0=Mon, 6=Sun)')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.title('Prediction Error by Day of Week')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNTC6YUFcHdL"
      },
      "outputs": [],
      "source": [
        "def fast_bootstrap_ci(best_pipeline, X_train, y_train, X_test, n_bootstrap=20):\n",
        "    \"\"\"Reduced bootstrap iterations for faster execution\"\"\"\n",
        "\n",
        "    logging.info(f\"Estimating confidence intervals with {n_bootstrap} bootstrap samples...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    bootstrap_predictions = []\n",
        "\n",
        "    for i in range(n_bootstrap):\n",
        "        if i % 5 == 0:\n",
        "            logging.info(f\"Bootstrap iteration {i+1}/{n_bootstrap}\")\n",
        "\n",
        "\n",
        "        bootstrap_indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
        "        X_boot = X_train.iloc[bootstrap_indices]\n",
        "        y_boot = y_train.iloc[bootstrap_indices]\n",
        "\n",
        "        # Clone the pipeline to avoid interference\n",
        "        from sklearn.base import clone\n",
        "        boot_pipeline = clone(best_pipeline)\n",
        "        boot_pipeline.fit(X_boot, y_boot)\n",
        "\n",
        "        # Predict on test set\n",
        "        boot_pred = boot_pipeline.predict(X_test)\n",
        "        bootstrap_predictions.append(boot_pred)\n",
        "\n",
        "    bootstrap_predictions = np.array(bootstrap_predictions)\n",
        "    execution_time = time.time() - start_time\n",
        "\n",
        "    logging.info(f\"Bootstrap confidence intervals completed in {execution_time:.2f} seconds\")\n",
        "    return bootstrap_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzL-SEBzcKeW"
      },
      "outputs": [],
      "source": [
        "# Calculate confidence intervals\n",
        "confidence_level = 0.95\n",
        "alpha = 1 - confidence_level\n",
        "lower_percentile = (alpha/2) * 100\n",
        "upper_percentile = (1 - alpha/2) * 100\n",
        "\n",
        "lower_bound = np.percentile(bootstrap_predictions, lower_percentile, axis=0)\n",
        "upper_bound = np.percentile(bootstrap_predictions, upper_percentile, axis=0)\n",
        "\n",
        "# Plot confidence intervals for a  predictions\n",
        "sample_indices = np.random.choice(len(y_test), size=100, replace=False)\n",
        "sample_indices = np.sort(sample_indices)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(range(len(sample_indices)), y_test.iloc[sample_indices], alpha=0.7, label='Actual', color='blue')\n",
        "plt.scatter(range(len(sample_indices)), y_pred_final[sample_indices], alpha=0.7, label='Predicted', color='red')\n",
        "plt.fill_between(range(len(sample_indices)),\n",
        "                 lower_bound[sample_indices],\n",
        "                 upper_bound[sample_indices],\n",
        "                 alpha=0.3, color='gray', label=f'{confidence_level*100}% Confidence Interval')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Predictions with Confidence Intervals (Sample)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WKtE9z-cNjA"
      },
      "outputs": [],
      "source": [
        "# Calculate coverage percentage\n",
        "coverage = np.mean((y_test >= lower_bound) & (y_test <= upper_bound))\n",
        "logging.info(f\"Confidence interval coverage: {coverage*100:.1f}%\")\n",
        "\n",
        "# 2.5 SERIALIZE MODELS\n",
        "logging.info(\"Serializing the best model...\")\n",
        "\n",
        "#  timestamp for model versioning\n",
        "timestamp = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S-%f\")[:-3]\n",
        "model_filename = f\"rossmann_model_{timestamp}.pkl\"\n",
        "\n",
        "# Save model with preprocessing pipeline and encoders\n",
        "model_package = {\n",
        "    'model': best_pipeline,\n",
        "    'feature_columns': feature_columns,\n",
        "    'label_encoders': {\n",
        "        'StoreType': le_store_type,\n",
        "        'Assortment': le_assortment,\n",
        "        'StateHoliday': le_state_holiday,\n",
        "        'PromoInterval': le_promo_interval\n",
        "    },\n",
        "    'model_metrics': {\n",
        "        'rmse': final_rmse,\n",
        "        'rmspe': final_rmspe,\n",
        "        'r2': final_r2\n",
        "    },\n",
        "    'training_date': timestamp\n",
        "}\n",
        "\n",
        "with open(model_filename, 'wb') as f:\n",
        "    pickle.dump(model_package, f)\n",
        "\n",
        "logging.info(f\"Model saved as {model_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMByvjGPcR1k"
      },
      "outputs": [],
      "source": [
        "# MLFlow logging\n",
        "logging.info(\"Logging to MLFlow...\")\n",
        "\n",
        "mlflow.set_experiment(\"Rossmann Sales Forecasting\")\n",
        "\n",
        "with mlflow.start_run():\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"model_type\", best_model_name)\n",
        "    mlflow.log_param(\"features_count\", len(feature_columns))\n",
        "    mlflow.log_param(\"training_samples\", len(X_train))\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"rmse\", final_rmse)\n",
        "    mlflow.log_metric(\"rmspe\", final_rmspe)\n",
        "    mlflow.log_metric(\"r2_score\", final_r2)\n",
        "    mlflow.log_metric(\"confidence_coverage\", coverage)\n",
        "\n",
        "    # Log model\n",
        "    mlflow.sklearn.log_model(best_pipeline, \"model\")\n",
        "\n",
        "    # Log artifacts\n",
        "    mlflow.log_artifact(model_filename)\n",
        "\n",
        "logging.info(\"MLFlow logging completed\")\n",
        "\n",
        "# Save predictions for analysis\n",
        "predictions_df = pd.DataFrame({\n",
        "    'Actual': y_test,\n",
        "    'Predicted': y_pred_final,\n",
        "    'Lower_CI': lower_bound,\n",
        "    'Upper_CI': upper_bound,\n",
        "    'Store': X_test['Store'],\n",
        "    'DayOfWeek': X_test['DayOfWeek']\n",
        "})\n",
        "\n",
        "predictions_df.to_csv(f'predictions_{timestamp}.csv', index=False)\n",
        "logging.info(f\"Predictions saved to predictions_{timestamp}.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMloE3SRcajj"
      },
      "outputs": [],
      "source": [
        "# Performance summary\n",
        "print(f\"\\n=== FINAL MODEL PERFORMANCE SUMMARY ===\")\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"RMSE: {final_rmse:.2f}\")\n",
        "print(f\"RMSPE: {final_rmspe:.3f}%\")\n",
        "print(f\"R² Score: {final_r2:.3f}\")\n",
        "print(f\"Confidence Interval Coverage: {coverage*100:.1f}%\")\n",
        "print(f\"Model saved as: {model_filename}\")\n",
        "\n",
        "logging.info(\"Machine Learning pipeline completed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5P_1gNWj_Pa7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}